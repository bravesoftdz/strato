To make a long story short, the full sequence of things isn't all that complicated: First you make a tokenizer, that strips whitespace and comments from the code, and figures out what sequence of names, values (literals) and operators is there. This is fed to a parser that tries to make sense of all this, building a structure of things the code is about (the abstract syntax tree). This is a good base to start a number of things. One is an interpreter that simulates a virtual machine to run the program, this allowed me to get a feel of the language, and to test the above tools. Another would be a compiler, but since my knowledge of assembler and optimizing is limited, and there are a number of very bright people working very hard on this for years, it makes more sense to 'transpile' to the input format of some tools already available.

This is where is gets really interesting. There's LLVMIR, and GCC has a number of options, but there's also JVM, and even C and JavaScript (we really should be calling it ECMAScript) are collecting fame for this specific use (Mozilla's asm.js, Emscripten, GLSL's for of C for GPGPU).

It's good to start with a plan, but knowing this up front and the slow progress I made in the beginning, I was easily overwhelmed with the work that waited ahead. So I didn't think too much about this. Especially about the steps beyond that: once you've got a nice set of demonstrational programs in the new language, a times comes when you let the tools work on a special program: a rewrite of them in the new language, effectively building itself, called bootstrapping. In the old days this would be done with the first interpreter hand-coded in a specific machine's assembler (or even byte-code), but in these modern cross-platform days you start from a known language.

But to go back on the above steps: About the tokenizer. I knew about the existance of yacc and lex, vaguely understand what they do, but I'm not in the business of theoretically or academically analyse the theoretics of programming languages, so I felt comfortable to write the tokenizer from scratch. I noticed I was able to do this with a look-ahead of a mere 3 characters. I'm not sure if this a consequence of my Pascal heritage, but I think this is a good thing and advanced look-ahead and backtracking would be much more complicated to construct.

About the parser. I thought up front about the importance of the output of the parser and guessed there could be some gain on performance to be made there, since you want your language tools themselves to be performant as well. So I thought of the tree as an array of nodes with a fixed length in memory, so persisting to and from file is as easy of saving and loading this a block of memory into this array. Each node would be of a type, and this type determined what was stored in the remaining fields, this way I was able to store the data in 8 32-bit numbers, using 32 bytes per node.

... (more later)